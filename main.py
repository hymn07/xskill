"""
main.py - æ™ºèƒ½å†…å®¹æƒ…æŠ¥ Agent ç³»ç»Ÿå…¥å£

æ ¸å¿ƒèŒè´£:
1. åè°ƒå„æ¨¡å—å®Œæˆ"æ„å›¾ -> æ•°æ® -> ç»“è®º"çš„é—­ç¯
2. æä¾›å‘½ä»¤è¡Œæ¥å£å’Œ API å…¥å£
3. ä»»åŠ¡ç¼–æ’ä¸ç”¨æˆ·äº¤äº’é€»è¾‘

ä½¿ç”¨æµç¨‹:
1. èº«ä»½æ˜ å°„ (QueryEngine)
2. è®¡ç®—æ•°æ®ç¼ºå£ (StorageManager)
3. è°ƒç”¨çˆ¬è™« (XScraper)
4. AI åŠ¨æ€åˆ†æ (AnalysisGenerator)
5. å¯¼å‡ºæŠ¥å‘Š (Exporter)
"""

import os
import asyncio
import argparse
from typing import Optional
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from core.discoverer import AccountDiscoverer
from core.query_engine import QueryEngine
from core.storage_manager import StorageManager
from core.exporter import Exporter
from core.scrapers import XScraper
from skills import AnalysisGenerator
from core.schema_generator import SchemaGenerator
from core.annotator import DynamicAnnotator


class XSkillAgent:
    """æ™ºèƒ½å†…å®¹æƒ…æŠ¥ Agent ä¸»æ§ç±»"""
    
    def __init__(self):
        self.discoverer = AccountDiscoverer()
        self.query_engine = QueryEngine(discoverer=self.discoverer)
        self.storage = StorageManager()
        self.exporter = Exporter(storage_manager=self.storage)
        
        # çˆ¬è™«éœ€è¦ auth_tokenï¼Œå»¶è¿Ÿåˆå§‹åŒ–
        self._scraper = None
        
        # åˆ†æå™¨
        self.analyzer = AnalysisGenerator()
    
    @property
    def scraper(self) -> Optional[XScraper]:
        """å»¶è¿Ÿåˆå§‹åŒ–çˆ¬è™«"""
        if self._scraper is None:
            auth_token = os.getenv("TWITTER_AUTH_TOKEN")
            if auth_token:
                self._scraper = XScraper(auth_token=auth_token)
            else:
                print("âš ï¸ æœªé…ç½® TWITTER_AUTH_TOKENï¼ŒæŠ“å–åŠŸèƒ½ä¸å¯ç”¨")
        return self._scraper
    
    async def run_pipeline(
        self,
        query: str,
        start_date: str = None,
        end_date: str = None,
        export: bool = True,
        analyze: bool = True
    ) -> dict:
        """
        æ‰§è¡Œå®Œæ•´çš„æƒ…æŠ¥è·å–æµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢ï¼Œå¦‚ "é©¬æ–¯å…‹æœ€è¿‘ä¸€å‘¨"
            start_date: èµ·å§‹æ—¥æœŸ (å¯é€‰ï¼Œä¼šå°è¯•ä» query è§£æ)
            end_date: ç»“æŸæ—¥æœŸ (å¯é€‰)
            export: æ˜¯å¦å¯¼å‡º Excel
            analyze: æ˜¯å¦è¿›è¡Œ AI åˆ†æ
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        print(f"\n{'='*50}")
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡: {query}")
        print(f"{'='*50}\n")
        
        result = {
            "query": query,
            "start_time": datetime.now().isoformat(),
            "steps": []
        }
        
        # Step 1: æ›´æ–°è´¦å·æ± 
        print("ğŸ“¡ Step 1: æ›´æ–°è´¦å·æ± ...")
        new_count, _ = self.discoverer.fetch_and_update()
        result["steps"].append({
            "name": "è´¦å·å‘ç°",
            "new_accounts": new_count
        })
        
        # Step 2: èº«ä»½è¯†åˆ«
        print("ğŸ” Step 2: è¯†åˆ«ç›®æ ‡...")
        identity = self.query_engine.identify_multiple(query)
        result["steps"].append({
            "name": "èº«ä»½è¯†åˆ«",
            "result": identity
        })
        
        if identity["mode"] == "none":
            print(f"âš ï¸ {identity['message']}")
            result["error"] = identity['message']
            return result
        
        handles = identity["handles"]
        print(f"âœ… è¯†åˆ«åˆ° {len(handles)} ä¸ªç›®æ ‡: {', '.join(handles[:5])}{'...' if len(handles) > 5 else ''}")
        
        # Step 3: è§£ææ—¶é—´èŒƒå›´
        if not start_date or not end_date:
            parsed_start, parsed_end = self.query_engine.parse_time_range(query)
            start_date = start_date or parsed_start
            end_date = end_date or parsed_end
        
        if not start_date:
            print(f"ğŸ’¡ {self.query_engine.ask_for_time_range()}")
            # é»˜è®¤ä½¿ç”¨æœ€è¿‘7å¤©
            from datetime import timedelta
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
            print(f"   ä½¿ç”¨é»˜è®¤èŒƒå›´: {start_date} è‡³ {end_date}")
        
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        
        # Step 4 & 5: é’ˆå¯¹æ¯ä¸ªç”¨æˆ·æŠ“å–æ•°æ®
        total_fetched = 0
        all_gaps = []
        
        print("ğŸ“Š Step 4 & 5: æ£€æŸ¥ç¼ºå£å¹¶æŠ“å–æ•°æ®...")
        for handle in handles:
            gaps = self.storage.get_missing_ranges(handle, start_date, end_date)
            if gaps:
                print(f"   [ @{handle} ] å‘ç° {len(gaps)} ä¸ªç¼ºå£åŒºé—´")
                if self.scraper:
                    for gap_start, gap_end in gaps:
                        print(f"      æŠ“å– {gap_start} è‡³ {gap_end}...")
                        tweets = await self.scraper.scrape(
                            handle, 
                            start_date=gap_start, 
                            end_date=gap_end,
                            count=100
                        )
                        
                        if tweets:
                            save_tweets = [{
                                "tweet_id": t["content_id"],
                                "author": t["author"],
                                "text": t["text"],
                                "created_at": t["publish_time"],
                                "url": t["url"],
                                "is_retweet": t["is_retweet"],
                                "metrics": t.get("metrics", {}),
                                "metadata": t.get("metadata", {})
                            } for t in tweets]
                            
                            saved = self.storage.save_tweets(save_tweets)
                            self.storage.update_manifest(handle, (gap_start, gap_end))
                            total_fetched += saved
                            print(f"      âœ… ä¿å­˜ {saved} æ¡æ¨æ–‡")
                all_gaps.extend([(handle, g) for g in gaps])
            else:
                print(f"   [ @{handle} ] âœ… æ— éœ€æŠ“å–")

        result["steps"].append({
            "name": "ç¼ºå£è®¡ç®—ä¸æŠ“å–",
            "total_fetched": total_fetched,
            "gaps_found": len(all_gaps)
        })
        
        # Step 6: è·å–å…¨é‡æœ¬åœ°æ•°æ®
        print("ğŸ“š Step 6: è¯»å–æœ¬åœ°æ±‡æ€»æ•°æ®...")
        data = self.storage.get_tweets(
            author=handles,
            start_date=start_date,
            end_date=end_date
        )
        result["data_count"] = len(data)
        print(f"   æ±‡æ€»å…± {len(data)} æ¡æ•°æ®")
        
        # Step 7: AI åˆ†æ (èšåˆåˆ†æ)
        annotated_data = data  # é»˜è®¤ä½¿ç”¨åŸå§‹æ•°æ®
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åŠ¨æ€æ ‡æ³¨ï¼ˆåŸºäº query æ„å›¾ï¼‰
        if "æ ‡æ³¨" in query or "çœ‹è®¨è®º" in query or "åˆ¤æ–­" in query:
            print("ğŸ·ï¸  æ­£åœ¨è¿›è¡Œå³æ—¶åŠ¨æ€æ ‡æ³¨...")
            schema_gen = SchemaGenerator()
            schema = await schema_gen.generate_from_user_intent(query)
            annotator = DynamicAnnotator(schema=schema, storage_manager=self.storage)
            # æ— çŠ¶æ€æ ‡æ³¨ï¼Œç›´æ¥è·å–ç»“æœ
            annotated_data = await annotator.annotate_all(author=handles)
            print(f"   âœ… å·²å®Œæˆ {len(annotated_data)} æ¡æ¨æ–‡çš„å³æ—¶æ ‡æ³¨")
        
        if analyze and annotated_data:
            print("ğŸ§  Step 8: AI èšåˆåˆ†æä¸­...")
            analysis = await self.analyzer.analyze(query, annotated_data)
            result["analysis"] = analysis
            
            if analysis.get("highlights"):
                print("   ğŸ“Œ é‡ç‚¹å‘ç°:")
                for h in analysis["highlights"][:3]:
                    print(f"      â€¢ {h}")
            
            # ä¿å­˜ AI åˆ†ææŠ¥å‘Šä¸º MD
            if 'error' not in analysis:
                report_path = self.analyzer.save_report(analysis)
                result["analysis_report_path"] = report_path
        
        # Step 9: å¯¼å‡ºèšåˆ Excel
        if export and annotated_data:
            print("ğŸ“ Step 9: å¯¼å‡ºèšåˆæŠ¥å‘Š...")
            filepath = self.exporter.export_to_excel(
                author=handles,
                start_date=start_date,
                end_date=end_date,
                external_data=annotated_data
            )
            result["export_path"] = filepath
        
        result["end_time"] = datetime.now().isoformat()
        print(f"\n{'='*50}")
        print("âœ… ä»»åŠ¡å®Œæˆ!")
        print(f"{'='*50}\n")
        
        return result
    
    def update_accounts(self) -> int:
        """ä»…æ›´æ–°è´¦å·æ± """
        new_count, _ = self.discoverer.fetch_and_update()
        return new_count
    
    def list_accounts(self):
        """åˆ—å‡ºæ‰€æœ‰è´¦å·"""
        accounts = self.discoverer.get_all_accounts()
        print(f"\nğŸ“‹ è´¦å·æ± å…±æœ‰ {len(accounts)} ä¸ªåšä¸»:\n")
        for acc in accounts:
            print(f"  â€¢ {acc['name']} (@{acc['screen_name']})")
            if acc.get('description'):
                print(f"    {acc['description'][:60]}...")
        print()


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½å†…å®¹æƒ…æŠ¥ Agent ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py "é©¬æ–¯å…‹æœ€è¿‘ä¸€å‘¨"
  python main.py "çœ‹çœ‹ sama æœ¬æœˆå‘äº†ä»€ä¹ˆ" --no-analyze
  python main.py --update-accounts
  python main.py --list-accounts
        """
    )
    
    parser.add_argument("query", nargs="?", help="æŸ¥è¯¢å†…å®¹ï¼Œå¦‚ 'é©¬æ–¯å…‹æœ€è¿‘ä¸€å‘¨'")
    parser.add_argument("--start", "-s", help="èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end", "-e", help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--no-export", action="store_true", help="ä¸å¯¼å‡º Excel")
    parser.add_argument("--no-analyze", action="store_true", help="ä¸è¿›è¡Œ AI åˆ†æ")
    parser.add_argument("--update-accounts", action="store_true", help="ä»…æ›´æ–°è´¦å·æ± ")
    parser.add_argument("--list-accounts", action="store_true", help="åˆ—å‡ºæ‰€æœ‰è´¦å·")
    
    args = parser.parse_args()
    
    agent = XSkillAgent()
    
    if args.update_accounts:
        new = agent.update_accounts()
        print(f"âœ… è´¦å·æ± æ›´æ–°å®Œæˆï¼Œæ–°å¢ {new} ä¸ªåšä¸»")
        return
    
    if args.list_accounts:
        agent.list_accounts()
        return
    
    if not args.query:
        parser.print_help()
        return
    
    # æ‰§è¡Œä¸»æµç¨‹
    result = asyncio.run(agent.run_pipeline(
        query=args.query,
        start_date=args.start,
        end_date=args.end,
        export=not args.no_export,
        analyze=not args.no_analyze
    ))
    
    if result.get("error"):
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {result['error']}")
    else:
        if result.get("export_path"):
            print(f"ğŸ“ Excel æŠ¥å‘Š: {result['export_path']}")
        if result.get("analysis_report_path"):
            print(f"ğŸ“ AI åˆ†ææŠ¥å‘Š: {result['analysis_report_path']}")


if __name__ == "__main__":
    main()

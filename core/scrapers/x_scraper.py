"""
x_scraper.py - Twitter/X å¹³å°çˆ¬è™«å®ç°

åŸºäº twikit åº“ï¼Œæ”¯æŒ Auth Token è®¤è¯å’Œæ—¥æœŸè¿‡æ»¤
æ•´åˆè‡ªåŸ zara.py ä¸­çš„ TwitterScraper ç±»
"""

import os
import asyncio
from typing import List, Dict, Optional
from datetime import datetime, timedelta

from twikit import Client

from .base_scraper import BaseScraper


class XScraper(BaseScraper):
    """Twitter/X çˆ¬è™«ï¼šåˆ©ç”¨ Auth Token æŠ“å–æ¨æ–‡"""
    
    def __init__(
        self, 
        auth_token: str = None, 
        ct0: str = None,
        language: str = 'en-US'
    ):
        super().__init__(platform="twitter")
        
        self.auth_token = auth_token or os.getenv("TWITTER_AUTH_TOKEN")
        self.ct0 = ct0 or os.getenv("TWITTER_CT0")
        
        if not self.auth_token:
            raise ValueError("éœ€è¦æä¾› auth_token æˆ–è®¾ç½® TWITTER_AUTH_TOKEN ç¯å¢ƒå˜é‡")
        
        self.client = Client(language)
        self.cookies = {
            'auth_token': self.auth_token,
            'ct0': self.ct0 if self.ct0 else 'dummy_ct0'
        }
        self._cookies_set = False
    
    async def _ensure_cookies(self):
        """ç¡®ä¿ cookies å·²è®¾ç½®"""
        if not self._cookies_set:
            #print(f"DEBUG: Setting Cookies: auth_token={self.cookies.get('auth_token')}, ct0={self.cookies.get('ct0')}")
            self.client.set_cookies(self.cookies)
            self._cookies_set = True
    
    async def scrape(
        self, 
        handle: str, 
        start_date: str = None, 
        end_date: str = None,
        count: int = 20,
        max_retries: int = 2,
        base_delay: float = 30.0
    ) -> List[Dict]:
        """
        æŠ“å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡ (ä½¿ç”¨ search_tweet æ›¿ä»£ get_user_tweets ä»¥æ”¯æŒæ›´çµæ´»çš„æ—¶é—´è¿‡æ»¤)
        
        Rate Limiting:
            - é‡åˆ° 429 é”™è¯¯æ—¶é‡è¯• 2 æ¬¡ (30s â†’ 60s)
            - æ¯æ¬¡è¯·æ±‚å‰æ·»åŠ éšæœºå»¶è¿Ÿ (3-5ç§’)
        """
        await self._ensure_cookies()
        
        # æ„é€ æŸ¥è¯¢è¯­å¥ from:user since:YYYY-MM-DD until:YYYY-MM-DD
        query = f"from:{handle}"
        
        # ä½¿ç”¨ä¼ å…¥çš„ start_dateï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤æœ€è¿‘30å¤©ï¼Œç¡®ä¿ query å®Œæ•´
        if not start_date:
            start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        query += f" since:{start_date}"
        
        # until å‚æ•°å¤„ç†ï¼šå¦‚æœä¸ä¼ ï¼Œé»˜è®¤åŒ…å«ä»Šå¤©ï¼ˆç›´åˆ°æ˜å¤©ï¼‰
        if not end_date:
            end_date = (datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
        
        # æ³¨æ„ï¼šsearch_tweet çš„ until æ˜¯ä¸åŒ…å«çš„
        query += f" until:{end_date}"
        
        print(f"ğŸ” æ‰§è¡Œæœç´¢: {query}")
        
        # é‡è¯•é€»è¾‘
        import random
        from twikit.errors import TooManyRequests
        
        for attempt in range(max_retries + 1):
            try:
                # æ¯æ¬¡è¯·æ±‚å‰æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹
                if attempt > 0 or hasattr(self, '_last_request_time'):
                    delay = random.uniform(3.0, 5.0)
                    await asyncio.sleep(delay)
                
                self._last_request_time = datetime.now()
                
                # ä½¿ç”¨ search_tweet (product='Latest' æŒ‰æ—¶é—´æ’åº)
                tweets = await self.client.search_tweet(query, product='Latest', count=count)
                
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                results = []
                if tweets:
                    for tweet in tweets:
                        # å°è¯•è§£æ URL
                        try:
                            url = f"https://x.com/{handle}/status/{tweet.id}"
                        except:
                            url = ""
                        
                        # å°è¯•å®‰å…¨è·å–å±æ€§
                        favorite_count = getattr(tweet, 'favorite_count', 0)
                        retweet_count = getattr(tweet, 'retweet_count', 0)
                        reply_count = getattr(tweet, 'reply_count', 0)
                        quote_count = getattr(tweet, 'quote_count', 0) # æ–°å¢å¼•ç”¨æ•°
                        view_count = getattr(tweet, 'view_count', 0)
                        if view_count is None: view_count = 0
                        lang = getattr(tweet, 'lang', '')
                        
                        # è·å–ä½œè€…ä¿¡æ¯ (tweet.user å±æ€§)
                        user_name = handle
                        followers_count = 0
                        if hasattr(tweet, 'user'):
                             user_name = getattr(tweet.user, 'name', handle)
                             followers_count = getattr(tweet.user, 'followers_count', 0)

                        content = {
                            "content_id": tweet.id,
                            "author": handle,
                            "author_name": user_name,
                            "text": tweet.text,
                            "publish_time": self._parse_twitter_time(tweet.created_at), # twikit è¿”å›çš„æ˜¯æ ¼å¼åŒ–å¥½çš„æ—¶é—´å­—ç¬¦ä¸²
                            "url": url,
                            "platform": "twitter",
                            "metrics": {
                                "likes": favorite_count,
                                "retweets": retweet_count,
                                "replies": reply_count,
                                "quotes": quote_count,
                                "views": view_count,
                            },
                            "lang": lang,
                            "is_retweet": str(tweet.text).startswith("RT @"), # ç®€å•åˆ¤æ–­
                            "metadata": {
                                "raw_created_at": str(tweet.created_at),
                                "author_followers": followers_count
                            }
                        }
                        results.append(content)
                
                # äºŒæ¬¡æ—¥æœŸè¿‡æ»¤ï¼ˆåŒé‡ä¿é™©ï¼Œä¸” search_tweet æœ‰æ—¶ä¸ç²¾å‡†ï¼‰
                # æ³¨æ„ï¼šfilter_by_date éœ€è¦ä¸ publish_time æ ¼å¼åŒ¹é…
                # è¿™é‡Œ publish_time æ˜¯ twikit çš„å­—ç¬¦ä¸²ï¼Œfilter_by_date å†…éƒ¨ä¼šè§£æ
                if start_date or end_date:
                    # ä¼ å…¥ç®€å•çš„æ—¥æœŸå­—ç¬¦ä¸²ç”¨äºæ¯”è¾ƒ
                    results = self.filter_by_date(results, start_date, end_date)
                
                return results
                
            except TooManyRequests as e:
                if attempt < max_retries:
                    # æŒ‡æ•°é€€é¿: 30s â†’ 60s
                    wait_time = base_delay * (2 ** attempt)
                    
                    # å°è¯•ä»å“åº”å¤´è·å–é‡ç½®æ—¶é—´
                    if hasattr(e, 'headers') and e.headers:
                        reset_time = e.headers.get('x-rate-limit-reset')
                        if reset_time:
                            try:
                                reset_dt = datetime.fromtimestamp(int(reset_time))
                                wait_time = max(wait_time, (reset_dt - datetime.now()).total_seconds() + 5)
                            except:
                                pass
                    
                    print(f"â³ è§¦å‘é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.0f} ç§’åé‡è¯• (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"âŒ æŠ“å– @{handle} å¤±è´¥: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                    return []
                    
            except Exception as e:
                print(f"âŒ æŠ“å– @{handle} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return []
        
        return []
    
    async def validate_credentials(self) -> bool:
        """éªŒè¯ Twitter å‡­æ®æ˜¯å¦æœ‰æ•ˆ"""
        await self._ensure_cookies()
        
        try:
            # å°è¯•è·å–è‡ªå·±çš„ä¿¡æ¯ä½œä¸ºéªŒè¯
            me = await self.client.get_user_by_screen_name("twitter")
            return me is not None
        except Exception as e:
            print(f"âŒ å‡­æ®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _parse_twitter_time(self, twitter_time) -> str:
        """
        è§£æ Twitter æ—¶é—´æ ¼å¼
        
        Twitter è¿”å›æ ¼å¼å¦‚: "Sat Jan 20 10:30:00 +0000 2024"
        """
        if isinstance(twitter_time, datetime):
            return twitter_time.isoformat()
        
        if isinstance(twitter_time, str):
            try:
                # å°è¯•æ ‡å‡† Twitter æ ¼å¼
                dt = datetime.strptime(
                    twitter_time, 
                    "%a %b %d %H:%M:%S %z %Y"
                )
                return dt.isoformat()
            except:
                pass
            
            # å°è¯• ISO æ ¼å¼
            try:
                dt = datetime.fromisoformat(twitter_time.replace("Z", "+00:00"))
                return dt.isoformat()
            except:
                pass
        
        # æ— æ³•è§£æï¼Œè¿”å›å½“å‰æ—¶é—´
        return datetime.now().isoformat()
    
    async def get_user_info(self, handle: str) -> Optional[Dict]:
        """è·å–ç”¨æˆ·è¯¦ç»†ä¿¡æ¯"""
        await self._ensure_cookies()
        
        try:
            user = await self.client.get_user_by_screen_name(handle)
            return {
                "id": user.id,
                "name": user.name,
                "screen_name": user.screen_name,
                "description": getattr(user, 'description', ''),
                "followers_count": getattr(user, 'followers_count', 0),
                "following_count": getattr(user, 'following_count', 0),
                "verified": getattr(user, 'verified', False)
            }
        except Exception as e:
            print(f"âŒ è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}")
            return None


# ==================== å…¼å®¹æ€§åˆ«å ====================
# ä¿æŒä¸åŸ zara.py çš„å…¼å®¹
TwitterScraper = XScraper


# ==================== æµ‹è¯•ä»£ç  ====================
if __name__ == "__main__":
    import asyncio
    
    async def test():
        # éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡: TWITTER_AUTH_TOKEN
        scraper = XScraper()
        
        # éªŒè¯å‡­æ®
        valid = await scraper.validate_credentials()
        print(f"å‡­æ®æœ‰æ•ˆ: {valid}")
        
        if valid:
            # æŠ“å–ç¤ºä¾‹
            tweets = await scraper.scrape("elonmusk", count=5)
            print(f"è·å–åˆ° {len(tweets)} æ¡æ¨æ–‡")
            for t in tweets[:2]:
                print(f"  - {t['text'][:50]}...")
    
    asyncio.run(test())
